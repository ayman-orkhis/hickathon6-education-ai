{"cells":[{"cell_type":"code","execution_count":null,"id":"22e92d33","metadata":{"id":"22e92d33"},"outputs":[],"source":["# ============================================================\n","# STEP 1: LOAD DATA\n","# ============================================================\n","import pandas as pd\n","import numpy as np\n","import warnings\n","warnings.filterwarnings('ignore')\n","a\n","print(\"ğŸ“¥ Loading data...\")\n","\n","# Local paths (change to Kaggle paths for submission)\n","X_train_raw = pd.read_csv('X_train.csv')\n","y_train = pd.read_csv('y_train.csv')\n","X_test_raw = pd.read_csv('X_test.csv')\n","\n","# Sample for fast local testing (remove for full run)\n","SAMPLE = 1000\n","np.random.seed(42)\n","idx = np.random.choice(len(X_train_raw), SAMPLE, replace=False)\n","X_train_raw = X_train_raw.iloc[idx].reset_index(drop=True)\n","\n","y_train = y_train.iloc[idx].reset_index(drop=True)\n","print(f\"Target zeros: {(y_train['MathScore']==0).mean()*100:.1f}%\")\n","\n","X_test_raw = X_test_raw.iloc[:SAMPLE].reset_index(drop=True)\n","print(f\"Train: {X_train_raw.shape}, Test: {X_test_raw.shape}\")\n","\n","\n","\n","# Save test IDs (use Unnamed: 0, not sequential)\n","test_ids = X_test_raw['Unnamed: 0'].values"]},{"cell_type":"code","execution_count":null,"id":"ebaf75d0","metadata":{"id":"ebaf75d0"},"outputs":[],"source":["# ============================================================\n","# STEP 2: REMOVE LEAKY COLUMNS + CLEAN DATA\n","# ============================================================\n","print(\"ğŸ§¹ Removing leaky columns...\")\n","\n","# These columns are 100% NaN in test = data leakage\n","leaky_patterns = ['math_q', '_average_score', '_total_timing']\n","leaky_cols = [c for c in X_train_raw.columns if any(p in c.lower() for p in leaky_patterns)]\n","print(f\"   Removing {len(leaky_cols)} leaky columns\")\n","\n","# Clean train\n","X_train_full = X_train_raw.drop(columns=['Unnamed: 0'] + leaky_cols, errors='ignore')\n","X_train_full = X_train_full.select_dtypes(include=[np.number]).astype(np.float32)\n","\n","# Clean test\n","X_test_full = X_test_raw.drop(columns=['Unnamed: 0'] + leaky_cols, errors='ignore')\n","X_test_full = X_test_full.select_dtypes(include=[np.number]).astype(np.float32)\n","\n","print(f\"   Final features: {X_train_full.shape[1]}\")"]},{"cell_type":"code","execution_count":null,"id":"765bfc81","metadata":{"id":"765bfc81"},"outputs":[],"source":["# ============================================================\n","# STEP 3: GPU-ACCELERATED ENSEMBLE WITH TARGET NORMALIZATION\n","# ============================================================\n","import catboost as cb\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from sklearn.metrics import roc_auc_score\n","import lightgbm as lgb\n","import xgboost as xgb\n","import gc\n","\n","print(\"ğŸš€ GPU-Accelerated Ensemble with Target Normalization...\")\n","\n","y = y_train['MathScore'].values.ravel()\n","is_zero = (y == 0).astype(int)\n","\n","# ============ STAGE 1: CATBOOST CLASSIFIER (GPU) ============\n","print(\"\\nğŸ“Š Stage 1: CatBoost Classifier (GPU)...\")\n","skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n","clf_oof = np.zeros(len(y))\n","clf_test = np.zeros(len(X_test_full))\n","\n","for fold, (tr_idx, val_idx) in enumerate(skf.split(X_train_full, is_zero)):\n","    clf = cb.CatBoostClassifier(iterations=100, depth=5, learning_rate=0.2,\n","                                 random_seed=42+fold, verbose=0,\n","                                 early_stopping_rounds=20)\n","    clf.fit(X_train_full.iloc[tr_idx], is_zero[tr_idx],\n","            eval_set=(X_train_full.iloc[val_idx], is_zero[val_idx]))\n","    clf_oof[val_idx] = clf.predict_proba(X_train_full.iloc[val_idx])[:, 1]\n","    clf_test += clf.predict_proba(X_test_full)[:, 1] / 3\n","    del clf\n","    gc.collect()\n","\n","print(f\"   Classifier AUC: {roc_auc_score(is_zero, clf_oof):.4f}\")\n","\n","# ============ STAGE 2: GPU REGRESSORS WITH TARGET NORMALIZATION ============\n","print(\"\\nğŸ”¬ Stage 2: GPU Regressors with Target Normalization...\")\n","\n","nz_mask = y > 0\n","X_nz = X_train_full[nz_mask].values.astype(np.float32)\n","y_nz = y[nz_mask]\n","X_test_arr = X_test_full.values.astype(np.float32)\n","\n","# Target normalization\n","y_mean = y_nz.mean()\n","y_std = y_nz.std()\n","y_nz_norm = (y_nz - y_mean) / y_std\n","print(f\"   Target stats: mean={y_mean:.2f}, std={y_std:.2f}\")\n","\n","kf = KFold(n_splits=3, shuffle=True, random_state=42)\n","\n","# --- MODEL 1: CatBoost Regressor (GPU) ---\n","print(\"\\n   ğŸ”¥ Training CatBoost Regressor (GPU)...\")\n","cb_oof = np.zeros(len(y_nz))\n","cb_test = np.zeros(len(X_test_arr))\n","\n","for fold, (tr_idx, val_idx) in enumerate(kf.split(X_nz)):\n","    cb_reg = cb.CatBoostRegressor(\n","        iterations=1500, depth=8, learning_rate=0.05,\n","        l2_leaf_reg=5, random_seed=42+fold,\n","        verbose=0, early_stopping_rounds=150\n","    )\n","    cb_reg.fit(X_nz[tr_idx], y_nz_norm[tr_idx], eval_set=(X_nz[val_idx], y_nz_norm[val_idx]))\n","    cb_oof[val_idx] = cb_reg.predict(X_nz[val_idx])\n","    cb_test += cb_reg.predict(X_test_arr) / 3\n","    del cb_reg\n","    gc.collect()\n","\n","cb_oof_denorm = cb_oof * y_std + y_mean\n","cb_test_denorm = cb_test * y_std + y_mean\n","cb_r2 = 1 - (np.sum((y_nz - cb_oof_denorm)**2) / np.sum((y_nz - y_nz.mean())**2))\n","print(f\"      CatBoost RÂ²: {cb_r2:.4f}\")\n","\n","# --- MODEL 2: LightGBM (GPU) ---\n","print(\"\\n   ğŸ”¥ Training LightGBM (GPU)...\")\n","lgb_oof = np.zeros(len(y_nz))\n","lgb_test = np.zeros(len(X_test_arr))\n","\n","X_nz_clean = np.nan_to_num(X_nz, nan=0.0, posinf=0.0, neginf=0.0)\n","X_test_clean = np.nan_to_num(X_test_arr, nan=0.0, posinf=0.0, neginf=0.0)\n","\n","for fold, (tr_idx, val_idx) in enumerate(kf.split(X_nz)):\n","    lgb_reg = lgb.LGBMRegressor(\n","        n_estimators=1500, learning_rate=0.05, max_depth=8,\n","        num_leaves=128, reg_alpha=0.1, reg_lambda=1.0,\n","        min_child_samples=20, subsample=0.8, colsample_bytree=0.8,\n","        n_jobs=-1,\n","        random_state=42+fold, verbose=-1\n","    )\n","    lgb_reg.fit(X_nz_clean[tr_idx], y_nz_norm[tr_idx],\n","                eval_set=[(X_nz_clean[val_idx], y_nz_norm[val_idx])],\n","                callbacks=[lgb.early_stopping(150, verbose=False)])\n","    lgb_oof[val_idx] = lgb_reg.predict(X_nz_clean[val_idx])\n","    lgb_test += lgb_reg.predict(X_test_clean) / 3\n","    del lgb_reg\n","    gc.collect()\n","\n","lgb_oof_denorm = lgb_oof * y_std + y_mean\n","lgb_test_denorm = lgb_test * y_std + y_mean\n","lgb_r2 = 1 - (np.sum((y_nz - lgb_oof_denorm)**2) / np.sum((y_nz - y_nz.mean())**2))\n","print(f\"      LightGBM RÂ²: {lgb_r2:.4f}\")\n","\n","# --- MODEL 3: XGBoost (GPU) ---\n","print(\"\\n   ğŸ”¥ Training XGBoost (GPU)...\")\n","xgb_oof = np.zeros(len(y_nz))\n","xgb_test = np.zeros(len(X_test_arr))\n","\n","for fold, (tr_idx, val_idx) in enumerate(kf.split(X_nz)):\n","    xgb_reg = xgb.XGBRegressor(\n","        n_estimators=1500, learning_rate=0.05, max_depth=8,\n","        reg_alpha=0.1, reg_lambda=1.0, subsample=0.8, colsample_bytree=0.8,\n","        n_jobs=-1,\n","        random_state=42+fold, verbosity=0,\n","        early_stopping_rounds=150\n","    )\n","    xgb_reg.fit(X_nz_clean[tr_idx], y_nz_norm[tr_idx],\n","                eval_set=[(X_nz_clean[val_idx], y_nz_norm[val_idx])], verbose=False)\n","    xgb_oof[val_idx] = xgb_reg.predict(X_nz_clean[val_idx])\n","    xgb_test += xgb_reg.predict(X_test_clean) / 3\n","    del xgb_reg\n","    gc.collect()\n","\n","xgb_oof_denorm = xgb_oof * y_std + y_mean\n","xgb_test_denorm = xgb_test * y_std + y_mean\n","xgb_r2 = 1 - (np.sum((y_nz - xgb_oof_denorm)**2) / np.sum((y_nz - y_nz.mean())**2))\n","print(f\"      XGBoost RÂ²: {xgb_r2:.4f}\")\n","\n","# ============ BLEND GPU MODELS ============\n","print(\"\\nğŸ“Š GPU Model Summary:\")\n","print(f\"   CatBoost:  RÂ²={cb_r2:.4f}\")\n","print(f\"   LightGBM:  RÂ²={lgb_r2:.4f}\")\n","print(f\"   XGBoost:   RÂ²={xgb_r2:.4f}\")\n","\n","total_r2 = cb_r2 + lgb_r2 + xgb_r2\n","w_cb = cb_r2 / total_r2\n","w_lgb = lgb_r2 / total_r2\n","w_xgb = xgb_r2 / total_r2\n","print(f\"\\nğŸ”— Weights: CB={w_cb:.3f}, LGB={w_lgb:.3f}, XGB={w_xgb:.3f}\")\n","\n","blend_oof_nz = w_cb * cb_oof_denorm + w_lgb * lgb_oof_denorm + w_xgb * xgb_oof_denorm\n","blend_test = w_cb * cb_test_denorm + w_lgb * lgb_test_denorm + w_xgb * xgb_test_denorm\n","\n","blend_r2 = 1 - (np.sum((y_nz - blend_oof_nz)**2) / np.sum((y_nz - y_nz.mean())**2))\n","print(f\"   Blend RÂ² (non-zero): {blend_r2:.4f}\")\n","\n","# Map back to full arrays\n","reg_oof = np.zeros(len(y))\n","reg_oof[nz_mask] = blend_oof_nz\n","\n","# ============ COMBINE WITH CLASSIFIER ============\n","final_oof = reg_oof.copy()\n","final_oof[clf_oof > 0.5] = 0\n","\n","final_test = blend_test.copy()\n","final_test[clf_test > 0.5] = 0\n","\n","oof_r2 = 1 - (np.sum((y - final_oof)**2) / np.sum((y - y.mean())**2))\n","print(f\"\\nğŸ¯ FINAL OOF RÂ² = {oof_r2:.4f}\")"]},{"cell_type":"code","execution_count":null,"id":"cb80a2f1","metadata":{"id":"cb80a2f1"},"outputs":[],"source":["# ============================================================\n","# STEP 4: GENERATE SUBMISSION\n","# ============================================================\n","print(\"ğŸ“¤ Generating submission...\")\n","\n","submission = pd.DataFrame({\n","    'ID': test_ids,\n","    'MathScore': final_test\n","})\n","\n","submission['MathScore'] = submission['MathScore'].clip(lower=0)\n","submission.to_csv('submission.csv', index=False)\n","\n","print(f\"âœ… Saved submission.csv\")\n","print(f\"   Shape: {submission.shape}\")\n","print(f\"   Zeros: {(submission['MathScore'] == 0).sum()} ({(submission['MathScore'] == 0).mean()*100:.1f}%)\")\n","print(f\"   Mean: {submission['MathScore'].mean():.2f}\")\n","print(f\"\\nğŸ† OOF RÂ² = {oof_r2:.4f}\")"]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}